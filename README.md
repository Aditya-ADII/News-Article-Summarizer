<div align="center">
  <h1><strong>ğŸ“° News Article Summarizer<br>(Mahakumbh Festival)</strong></h1>
  <p><em>Automated abstractive summarization of Indian news coverage using T5 Transformers</em></p>
</div>

---

## ğŸ“Œ Project Overview

This project focuses on the **automated summarization** of news articles related to the **Mahakumbh Festival**, one of the largest religious gatherings in the world. The goal is to condense long news articles into concise, meaningful summaries using **state-of-the-art NLP techniques**.

The pipeline includes:
- **Data scraping** from Indian news sources
- **Cleaning and preprocessing** text
- **Training a transformer-based model (T5)** for summarization
- **Evaluating** performance using industry-standard metrics like ROUGE

---

## ğŸ§­ Motivation

In the era of information overload, it's crucial to extract relevant content quickly. Manually summarizing articles is time-consuming, especially for large datasets. This project demonstrates how **abstractive summarization** using deep learning can help automate this process with high-quality results. We chose the **Mahakumbh Festival** due to its cultural relevance and rich media coverage in India.

---

## âœ… Task Breakdown

### ğŸ”¹ Task 1: Dataset Collection
- **Source:** Articles were scraped from *The Indian Express* website.
- **Tools Used:**  
  - [`Selenium`](https://www.selenium.dev/) for dynamic browser automation  
  - [`BeautifulSoup`](https://www.crummy.com/software/BeautifulSoup/) for parsing HTML and extracting content
- **Output:** Extracted article titles, dates, and summaries; saved as structured data in `mahakumbh_articles.csv`.

---

### ğŸ”¹ Task 2: Dataset Annotation
- **Text Cleaning:** Removed noisy HTML elements, fixed encoding issues, and normalized text.
- **Data Preparation:** Combined article titles and content for model input. The corresponding original summaries served as ground truth labels.
- **Annotation Format:**  
  - `Input`: Cleaned title + article content  
  - `Target`: Official article summary

---

### ğŸ”¹ Task 3: Model Development & Summarization

#### ğŸ§  Model
- **Architecture:** [T5-base](https://huggingface.co/t5-base) â€“ a powerful encoder-decoder model for sequence-to-sequence tasks.
- **Framework:** [Hugging Face Transformers](https://huggingface.co/transformers/) and `Seq2SeqTrainer` for training and evaluation.

#### âš™ï¸ Pipeline
- Tokenized and split dataset into training and validation sets.
- Trained the model on news data using GPU-accelerated fine-tuning.
- Tracked:
  - **Training Loss**
  - **Validation Loss**
  - **ROUGE Scores** (ROUGE-1, ROUGE-2, ROUGE-L)

#### ğŸ“¤ Output
- Generated abstractive summaries for unseen inputs.
- Final results saved to `outputT5_fixed.csv`.

---

## ğŸ“ˆ Evaluation Metrics

Used **ROUGE** scores to evaluate the quality of generated summaries:
- **ROUGE-1**: Overlap of unigrams
- **ROUGE-2**: Overlap of bigrams
- **ROUGE-L**: Longest common subsequence

These metrics help quantify how close the generated summaries are to the human-written ones.

---

## ğŸ§‘â€ğŸ’» Contributors & Work Distribution

| Name                      | Roll No.   | Contribution                                                                 |
|---------------------------|------------|------------------------------------------------------------------------------|
| **Aditya Vilasrao Bhagat** | 2411AI27   | Scraped and collected article data using Selenium and BeautifulSoup. Handled preprocessing and structured storage of the dataset. |
| **Divyanshu Singh**        | 2411AI41   | Led model development: implemented tokenization, prepared data pipeline, set up training and validation. |
| **Vaibhav Shikhar Singh**  | 2411AI48   | Focused on training and evaluation. Handled ROUGE scoring and performance tracking of the model. |

---

## ğŸ› ï¸ Tech Stack

- **Languages:** Python  
- **Libraries & Tools:**  
  - `Selenium` â€“ Web scraping  
  - `BeautifulSoup` â€“ HTML parsing  
  - `Pandas`, `Numpy` â€“ Data handling  
  - `Transformers`, `Datasets` â€“ Hugging Face NLP models  
  - `Seq2SeqTrainer` â€“ For training T5  
  - `scikit-learn`, `ROUGE` â€“ Model evaluation

---

## ğŸ“ Project Structure

News-Article-Summarizer/
â”‚
â”œâ”€â”€ README.md # Project overview, tasks, contributors, and instructions
â”œâ”€â”€ mahakumbh_articles.csv # Raw dataset scraped from Indian Express (title, date, summary)
â”œâ”€â”€ outputT5_fixed.csv # Final generated summaries saved after model inference
â”‚
â”œâ”€â”€ scraping_script.py # Web scraping logic using Selenium and BeautifulSoup
â”œâ”€â”€ preprocessing.py # Text cleaning, annotation creation, and data formatting
â”œâ”€â”€ model_training.py # Training pipeline using T5-base and Hugging Face Transformers
â”‚
â”œâ”€â”€ requirements.txt # List of required libraries and dependencies
â”‚
â”œâ”€â”€ utils/
â”‚ â”œâ”€â”€ clean_text.py # Text normalization and cleaning helper
â”‚ â”œâ”€â”€ summary_generation.py # Script to generate summaries using the trained model
â”‚
â””â”€â”€ notebooks/
â”œâ”€â”€ scraping_demo.ipynb # Optional notebook to run the scraping process
â”œâ”€â”€ training_pipeline.ipynb # Model training and evaluation in interactive format
â””â”€â”€ inference.ipynb # Notebook to test and generate summaries interactively


## ğŸ§© Description of Key Components

- **`scraping_script.py`**  
  Scrapes Mahakumbh-related articles from The Indian Express using headless Chrome with Selenium and parses HTML using BeautifulSoup.

- **`preprocessing.py`**  
  Cleans scraped text, merges titles and summaries into article bodies, and formats them for training.

- **`model_training.py`**  
  Fine-tunes the T5-base model using Hugging Face's `Seq2SeqTrainer`, logs ROUGE scores, and saves final model and outputs.

- **`utils/`**  
  Contains modular helper scripts:
  - `clean_text.py`: Removes noisy characters and formats strings
  - `summary_generation.py`: Loads the trained model to generate abstractive summaries

- **`notebooks/`**  
  Contains Jupyter notebooks for experimentation, visualization, and testing the summarizer in an interactive way.

- **`mahakumbh_articles.csv`**  
  Output of the scraping and preprocessing pipeline. Contains article titles, publication dates, and original summaries.

- **`outputT5_fixed.csv`**  
  Contains final generated summaries by the model along with their source articles.

---

Let me know if youâ€™d like me to generate:
- A `requirements.txt` file  
- The `.py` files mentioned above  
- A zip of this entire folder structure

I can create a full-ready project scaffold for GitHub if needed!

